import torch
from torch import nn
from d2l import torch as d2l

net = nn.Sequential(nn.Flatten(),
                    nn.Linear(784, 256),
                    nn.ReLU(),
                    nn.Linear(256, 10))

def init_weights(m):
    if type(m) == nn.Linear:
        nn.init.normal_(m.weight, std=0.01)
# è¿™ä¸ªå‡½æ•°æ˜¯åœ¨åˆå§‹åŒ–æ¨¡å‹å‚æ•°ï¼šå°†æ‰€æœ‰çº¿æ€§å±‚çš„æƒé‡åˆå§‹åŒ–ä¸ºå‡å€¼ä¸º0ï¼Œæ ‡å‡†å·®ä¸º0.01çš„æ­£æ€åˆ†å¸ƒéšæœºæ•°
# è¿™ä¸ªå‡½æ•°ä¼šè¢«applyæ–¹æ³•ä½¿ç”¨ï¼Œä½œç”¨æ˜¯åˆå§‹åŒ–æ¯ä¸€å±‚çš„æƒé‡
# mæ˜¯ä¼ å…¥çš„æ¯ä¸€å±‚

net.apply(init_weights)
# ä½¿ç”¨applyæ–¹æ³•å°†init_weightså‡½æ•°åº”ç”¨åˆ°netçš„æ¯ä¸€å±‚

batch_size, lr, num_epochs = 256, 0.1, 10 # æ‰¹é‡å¤§å°ã€å­¦ä¹ ç‡ã€è¿­ä»£å‘¨æœŸ
loss = nn.CrossEntropyLoss(reduction='none') # äº¤å‰ç†µæŸå¤±å‡½æ•° reduction='none'è¡¨ç¤ºä¸å¯¹æŸå¤±å€¼åšä»»ä½•ç¼©å‡
# SGD (Stochastic Gradient Descent) - éšæœºæ¢¯åº¦ä¸‹é™
# ç‰¹ç‚¹ï¼šä½¿ç”¨å›ºå®šå­¦ä¹ ç‡ï¼Œç›´æ¥æŒ‰æ¢¯åº¦æ–¹å‘æ›´æ–°å‚æ•°
# å…¬å¼ï¼šÎ¸ = Î¸ - lr * âˆ‡Î¸
trainer = torch.optim.SGD(net.parameters(), lr=lr)

# Adam (Adaptive Moment Estimation) - è‡ªé€‚åº”çŸ©ä¼°è®¡
# ç‰¹ç‚¹ï¼šè‡ªé€‚åº”å­¦ä¹ ç‡ï¼Œç»“åˆäº†åŠ¨é‡å’ŒRMSpropçš„ä¼˜ç‚¹
# ç»´æŠ¤æ¯ä¸ªå‚æ•°çš„ä¸€é˜¶çŸ©(åŠ¨é‡)å’ŒäºŒé˜¶çŸ©(æ¢¯åº¦å¹³æ–¹çš„ç§»åŠ¨å¹³å‡)
# é€šå¸¸æ”¶æ•›æ›´å¿«ï¼Œå¯¹è¶…å‚æ•°ä¸æ•æ„Ÿ
# trainer = torch.optim.Adam(net.parameters(), lr=lr)

train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)

from MLP import train_ch3

if __name__ == '__main__':
    print("ğŸš€ å¼€å§‹è®­ç»ƒç®€åŒ–ç‰ˆMLP...")
    print(f"ğŸ“Š æ¨¡å‹ç»“æ„:\n{net}")
    print(f"ğŸ“ˆ å‚æ•°æ•°é‡: {sum(p.numel() for p in net.parameters()):,}")
    print("-" * 50)
    
    train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)
    print("\nâœ… è®­ç»ƒå®Œæˆï¼")
