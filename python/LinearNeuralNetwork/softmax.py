import torch
from IPython import display
from d2l import torch as d2l

batch_size = 256
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)
# ä½¿ç”¨ è¿™æœ¬ä¹¦çš„å·¥å…·åŒ… d2l æ¥åŠ è½½æ•°æ®é›†
# å®ƒå®Œæˆäº†æ•°æ®çš„é¢„å¤„ç†ï¼ˆåŒ…æ‹¬å½’ä¸€åŒ–å’Œæ ¼å¼è½¬æ¢ï¼‰ï¼Œåˆ†å‰²è®­ç»ƒé›†å’Œæµ‹è¯•é›†
# å¹¶åˆ›å»ºäº† PyTorch çš„æ•°æ®è¿­ä»£å™¨

def softmax(X):
    X_exp = torch.exp(X)
    partition = X_exp.sum(1, keepdim=True)
    # é€šè¿‡ keepdim=True ä¿æŒ sum åçš„å¼ é‡ç»´åº¦ä¸å˜ï¼šæ¯”å¦‚ä¸åŠ æœ¬æ¥æ˜¯[2,3], é€šè¿‡keepdimåå˜ä¸º[[2],[3]]
    return X_exp / partition  # å¹¿æ’­ï¼šä½¿å¾—åˆ†æ¯çš„å½¢çŠ¶ä¸ X_exp ç›¸åŒ
# è¿™æ ·ç®—å‡ºçš„æ˜¯X_expçš„æ¯ä¸€è¡Œé™¤ä»¥è¯¥è¡Œçš„å’Œ

y = torch.tensor([0, 2]) # 2ä¸ªæ ·æœ¬çš„æ ‡ç­¾
y_hat = torch.tensor([[0.1, 0.3, 0.6], [0.3, 0.2, 0.5]]) # 2ä¸ªæ ·æœ¬çš„é¢„æµ‹æ¦‚ç‡

def cross_entropy(y_hat, y):
    return - torch.log(y_hat[range(len(y_hat)), y])

def accuracy(y_hat, y):
    """è®¡ç®—é¢„æµ‹æ­£ç¡®çš„æ•°é‡"""
    if len(y_hat.shape) > 1 and y_hat.shape[1] > 1 :  # len(y_hat.shape)> 1 è¯´æ˜æœ‰å¤šåˆ—
        # å¦‚æœy_hatçš„åˆ—æ•°å¤§äº1ï¼Œæˆ‘ä»¬å°±å–æ¯ä¸€è¡Œçš„æœ€å¤§å€¼ä½œä¸ºé¢„æµ‹ç±»åˆ«
        y_hat = y_hat.argmax(axis=1)
    cmp = y_hat.type(y.dtype) == y # ä¸ºä»€ä¹ˆè¦è¿™æ ·å†™ï¼šå› ä¸ºy_hatçš„ç±»å‹å¯èƒ½æ˜¯floatï¼Œè€Œyçš„ç±»å‹æ˜¯int
    return float(cmp.type(y.dtype).sum()) # cmpæ˜¯ä¸€ä¸ª0-1å¼ é‡ï¼Œè¡¨ç¤ºæ¯ä¸ªé¢„æµ‹æ˜¯å¦æ­£ç¡® æ¯”å¦‚ [1,0,1,1,0]ï¼ŒæŠŠå®ƒè½¬æ¢æˆå’Œyä¸€æ ·çš„ç±»å‹ï¼Œç„¶åæ±‚å’Œ

# è®­ç»ƒsoftmaxå›å½’æ¨¡å‹

class Accumulator:
    """
    ä¸€ä¸ªç”¨äºåœ¨å¤šä¸ªå˜é‡ä¸Šè¿›è¡Œç´¯åŠ æ“ä½œçš„å·¥å…·ç±»ã€‚
    è¯¥ç±»å¯ä»¥ç»´æŠ¤nä¸ªæ•°å€¼å˜é‡ï¼Œæ”¯æŒæ‰¹é‡ç´¯åŠ ã€é‡ç½®å’Œç´¢å¼•è®¿é—®æ“ä½œã€‚
    å¸¸ç”¨äºæœºå™¨å­¦ä¹ ä¸­ç´¯è®¡æŸå¤±å€¼ã€å‡†ç¡®ç‡ç­‰å¤šä¸ªæŒ‡æ ‡çš„ç»Ÿè®¡ã€‚
    Attributes:
        data (list): å­˜å‚¨ç´¯åŠ å€¼çš„åˆ—è¡¨
    Example:
        >>> acc = Accumulator(3)  # åˆ›å»º3ä¸ªç´¯åŠ å™¨
        >>> acc.add(1, 2, 3)      # ç´¯åŠ  [1, 2, 3]
        >>> acc.add(4, 5, 6)      # å†ç´¯åŠ  [4, 5, 6]
        >>> print(acc[0])         # è¾“å‡º: 5.0 (1+4)
        >>> acc.reset()           # é‡ç½®æ‰€æœ‰ç´¯åŠ å™¨ä¸º0
        åˆå§‹åŒ–ç´¯åŠ å™¨ã€‚
        Args:
            n (int): éœ€è¦ç´¯åŠ çš„å˜é‡ä¸ªæ•°
        å¯¹æ‰€æœ‰å˜é‡è¿›è¡Œç´¯åŠ æ“ä½œã€‚
        Args:
            *args: è¦ç´¯åŠ çš„æ•°å€¼ï¼Œæ•°é‡åº”ä¸åˆå§‹åŒ–æ—¶çš„nç›¸åŒ
        é‡ç½®æ‰€æœ‰ç´¯åŠ å™¨çš„å€¼ä¸º0.0ã€‚
        é€šè¿‡ç´¢å¼•è·å–æŒ‡å®šä½ç½®çš„ç´¯åŠ å€¼ã€‚
        Args:
            idx (int): è¦è·å–çš„ç´¯åŠ å™¨ç´¢å¼•
        Returns:
            float: æŒ‡å®šä½ç½®çš„ç´¯åŠ å€¼
            åœ¨nä¸ªå˜é‡ä¸Šç´¯åŠ """
    def __init__(self, n):
        self.data = [0.0] * n

    def add(self, *args):
        self.data = [a + float(b) for a, b in zip(self.data, args)]

    def reset(self):
        self.data = [0.0] * len(self.data)

    def __getitem__(self, idx):
        return self.data[idx]

def train_epoch_ch3(net, train_iter, loss, updater):  #@save
    """è®­ç»ƒæ¨¡å‹ä¸€ä¸ªè¿­ä»£å‘¨æœŸ"""
    # å°†æ¨¡å‹è®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼
    if isinstance(net, torch.nn.Module):
        net.train()
    # è®­ç»ƒæŸå¤±æ€»å’Œã€è®­ç»ƒå‡†ç¡®åº¦æ€»å’Œã€æ ·æœ¬æ•°
    metric = Accumulator(3)
    for X, y in train_iter:
        # è®¡ç®—æ¢¯åº¦å¹¶æ›´æ–°å‚æ•°
        y_hat = net(X)
        l = loss(y_hat, y)
        if isinstance(updater, torch.optim.Optimizer):
            # ä½¿ç”¨PyTorchå†…ç½®çš„ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
            updater.zero_grad()
            l.mean().backward()
            updater.step()
        else:
            # ä½¿ç”¨è‡ªå®šä¹‰ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
            l.sum().backward()
            updater(X.shape[0])  # è°ƒç”¨è‡ªå®šä¹‰çš„sgdå‡½æ•°
        metric.add(float(l.sum()), accuracy(y_hat, y), y.numel())
# ### æƒ…å†µ1: PyTorchå†…ç½®ä¼˜åŒ–å™¨å¯¹è±¡
# ```python
# # åˆ›å»ºä¼˜åŒ–å™¨å¯¹è±¡
# optimizer = torch.optim.SGD([W, b], lr=0.1)
# train_ch3(..., optimizer)  # updater = optimizer

# # æ­¤æ—¶ï¼š
# isinstance(optimizer, torch.optim.Optimizer)  # è¿”å› True
# # å› ä¸º torch.optim.SGD ç»§æ‰¿è‡ª torch.optim.Optimizer
# ```

# ### æƒ…å†µ2: è‡ªå®šä¹‰å‡½æ•°ï¼ˆä½ çš„æƒ…å†µï¼‰
# ```python
# # å®šä¹‰å‡½æ•°
# def sgd(batch_size):
#     global W, b
#     with torch.no_grad():
#         W -= lr * W.grad / batch_size
#         b -= lr * b.grad / batch_size
#         W.grad.zero_()
#         b.grad.zero_()

# train_ch3(..., sgd)  # updater = sgd

# # æ­¤æ—¶ï¼š
# isinstance(sgd, torch.optim.Optimizer)  # è¿”å› False
# # å› ä¸º sgd æ˜¯ä¸€ä¸ªå‡½æ•°ï¼Œä¸æ˜¯ Optimizer ç±»çš„å®ä¾‹
# ```

# ## ğŸ’¡ ä¸ºä»€ä¹ˆè¿™æ ·è®¾è®¡ï¼Ÿ
# ```python
# # ä¸ç®¡ updater æ˜¯ä»€ä¹ˆç±»å‹ï¼Œåªè¦å®ƒèƒ½"æ›´æ–°å‚æ•°"å°±è¡Œ

# if isinstance(updater, torch.optim.Optimizer):
#     # å¦‚æœæ˜¯ä¼˜åŒ–å™¨å¯¹è±¡ï¼Œç”¨å¯¹è±¡çš„æ–¹æ³•
#     updater.zero_grad()  # æ¸…é›¶æ¢¯åº¦
#     updater.step()       # æ›´æ–°å‚æ•°
# else:
#     # å¦‚æœæ˜¯å‡½æ•°ï¼Œç›´æ¥è°ƒç”¨å‡½æ•°
#     updater(batch_size)  # è°ƒç”¨ sgd(batch_size)
# ```

# ## ğŸ¯ ç±»å‹æ£€æŸ¥ç¤ºä¾‹

# ```python
# import torch

# # ç¤ºä¾‹1: ä¼˜åŒ–å™¨å¯¹è±¡
# W = torch.randn(2, 3, requires_grad=True)
# optimizer = torch.optim.SGD([W], lr=0.1)
# print(isinstance(optimizer, torch.optim.Optimizer))  # True
# print(type(optimizer))  # <class 'torch.optim.sgd.SGD'>

# # ç¤ºä¾‹2: å‡½æ•°
# def my_sgd(batch_size):
#     pass

# print(isinstance(my_sgd, torch.optim.Optimizer))  # False
# print(type(my_sgd))  # <class 'function'>

# # ç¤ºä¾‹3: ç»§æ‰¿å…³ç³»
# print(isinstance(optimizer, torch.optim.SGD))      # True
# print(isinstance(optimizer, torch.optim.Optimizer)) # True (ç»§æ‰¿å…³ç³»)
# ```

# ## âœ… æ€»ç»“

# - **`isinstance()`**: æ£€æŸ¥å¯¹è±¡æ˜¯å¦å±äºæŒ‡å®šç±»å‹
# - **`updater`**: å‚æ•°æ›´æ–°å™¨ï¼Œå¯ä»¥æ˜¯ä¼˜åŒ–å™¨å¯¹è±¡æˆ–å‡½æ•°ï¼Œ**ä¸æ˜¯è¿­ä»£å™¨**
# - **ä½œç”¨**: è®©åŒä¸€ä¸ªè®­ç»ƒå‡½æ•°æ”¯æŒä¸¤ç§ä¸åŒçš„å‚æ•°æ›´æ–°æ–¹å¼
    # è¿”å›è®­ç»ƒæŸå¤±å’Œè®­ç»ƒç²¾åº¦
    return metric[0] / metric[2], metric[1] / metric[2]

def evaluate_accuracy(net, data_iter):  #@save
    """è®¡ç®—åœ¨æŒ‡å®šæ•°æ®é›†ä¸Šæ¨¡å‹çš„ç²¾åº¦"""
    if isinstance(net, torch.nn.Module):
        net.eval()  # å°†æ¨¡å‹è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
    metric = Accumulator(2)  # æ­£ç¡®é¢„æµ‹æ•°ã€é¢„æµ‹æ€»æ•°
    with torch.no_grad():
        for X, y in data_iter:
            metric.add(accuracy(net(X), y), y.numel())
    return metric[0] / metric[1]

def train_ch3(net, train_iter, test_iter, loss, num_epochs, updater):  #@save
    """è®­ç»ƒæ¨¡å‹"""
    for epoch in range(num_epochs):
        train_metrics = train_epoch_ch3(net, train_iter, loss, updater)
        test_acc = evaluate_accuracy(net, test_iter)
        print(f'epoch {epoch + 1}, loss {train_metrics[0]:.3f}, '
              f'train acc {train_metrics[1]:.3f}, test acc {test_acc:.3f}')

# å®šä¹‰æ¨¡å‹å‚æ•°
num_inputs = 784  # 28*28çš„å›¾åƒå±•å¹³åçš„ç‰¹å¾æ•°
num_outputs = 10  # 10ä¸ªç±»åˆ«ï¼ˆ0-9ï¼‰

# åˆå§‹åŒ–æ¨¡å‹å‚æ•°
W = torch.normal(0, 0.01, size=(num_inputs, num_outputs), requires_grad=True)
b = torch.zeros(num_outputs, requires_grad=True)

# å®šä¹‰æ¨¡å‹
def net(X):
    """softmaxå›å½’æ¨¡å‹"""
    return softmax(torch.matmul(X.reshape((-1, W.shape[0])), W) + b)

# å®šä¹‰æŸå¤±å‡½æ•°
def cross_entropy_loss(y_hat, y):
    """äº¤å‰ç†µæŸå¤±å‡½æ•°"""
    return cross_entropy(y_hat, y).mean()

# å®šä¹‰ä¼˜åŒ–å™¨ï¼ˆæ‰‹åŠ¨å®ç°SGDï¼‰
lr = 0.1  # å­¦ä¹ ç‡

def sgd(batch_size):
    """å°æ‰¹é‡éšæœºæ¢¯åº¦ä¸‹é™"""
    global W, b  # å£°æ˜ä½¿ç”¨å…¨å±€å˜é‡
    with torch.no_grad():
        W -= lr * W.grad / batch_size
        b -= lr * b.grad / batch_size
        W.grad.zero_()
        b.grad.zero_()

# å®é™…å·¥ä½œä¸­å¸¸ç”¨çš„PyTorchä¼˜åŒ–å™¨ç¤ºä¾‹

# # 1. SGDä¼˜åŒ–å™¨ï¼ˆéšæœºæ¢¯åº¦ä¸‹é™ï¼‰
# optimizer_sgd = torch.optim.SGD([W, b], lr=0.1, momentum=0.9, weight_decay=1e-4)

# # 2. Adamä¼˜åŒ–å™¨ï¼ˆæœ€å¸¸ç”¨ï¼Œè‡ªé€‚åº”å­¦ä¹ ç‡ï¼‰
# optimizer_adam = torch.optim.Adam([W, b], lr=0.001, betas=(0.9, 0.999), weight_decay=1e-4)

# # 3. AdamWä¼˜åŒ–å™¨ï¼ˆæ”¹è¿›çš„Adamï¼Œæ›´å¥½çš„æƒé‡è¡°å‡ï¼‰
# optimizer_adamw = torch.optim.AdamW([W, b], lr=0.001, weight_decay=0.01)

# # 4. RMSpropä¼˜åŒ–å™¨
# optimizer_rmsprop = torch.optim.RMSprop([W, b], lr=0.01, alpha=0.99)

# # å®é™…è®­ç»ƒæ—¶çš„æ ‡å‡†æµç¨‹ï¼š
# def standard_training_loop():
#     """å®é™…å·¥ä½œä¸­çš„æ ‡å‡†è®­ç»ƒå¾ªç¯"""
#     # é€‰æ‹©ä¼˜åŒ–å™¨ï¼ˆæœ€å¸¸ç”¨Adamï¼‰
#     optimizer = torch.optim.Adam([W, b], lr=0.001)
    
#     for epoch in range(num_epochs):
#         for X, y in train_iter:
#             # 1. å‰å‘ä¼ æ’­
#             y_hat = net(X)
#             loss = cross_entropy_loss(y_hat, y)
            
#             # 2. æ¸…é›¶æ¢¯åº¦ï¼ˆé‡è¦ï¼ï¼‰
#             optimizer.zero_grad()
            
#             # 3. åå‘ä¼ æ’­
#             loss.backward()
            
#             # 4. æ›´æ–°å‚æ•°
#             optimizer.step()

# æµ‹è¯•ä¸€äº›é¢„æµ‹
def predict(net, test_iter, n=6):
    """é¢„æµ‹å¹¶æ˜¾ç¤ºç»“æœ"""
    print(f"\né¢„æµ‹ç¤ºä¾‹ï¼ˆå‰{n}ä¸ªæ ·æœ¬ï¼‰:")
    print("-" * 30)
    for X, y in test_iter:
        trues = d2l.get_fashion_mnist_labels(y)
        preds = d2l.get_fashion_mnist_labels(net(X).argmax(axis=1))
        for i in range(min(n, len(trues))):
            print(f"çœŸå®: {trues[i]:>12}, é¢„æµ‹: {preds[i]:>12}")
        break

# ä¸»ç¨‹åº - è§£å†³Windowså¤šè¿›ç¨‹é—®é¢˜
if __name__ == '__main__':
    # å¼€å§‹è®­ç»ƒ
    print("å¼€å§‹è®­ç»ƒsoftmaxå›å½’æ¨¡å‹...")
    print(f"è®­ç»ƒé›†å¤§å°: {len(train_iter)} æ‰¹æ¬¡")
    print(f"æµ‹è¯•é›†å¤§å°: {len(test_iter)} æ‰¹æ¬¡")
    print(f"æ‰¹æ¬¡å¤§å°: {batch_size}")
    print(f"å­¦ä¹ ç‡: {lr}")
    print("-" * 50)

    num_epochs = 10
    train_ch3(net, train_iter, test_iter, cross_entropy_loss, num_epochs, sgd)

    print("-" * 50)
    print("è®­ç»ƒå®Œæˆï¼")

    # æ˜¾ç¤ºæœ€ç»ˆå‚æ•°
    print(f"\næœ€ç»ˆå‚æ•°å½¢çŠ¶:")
    print(f"æƒé‡ W: {W.shape}")
    print(f"åç½® b: {b.shape}")

    # ä¿å­˜è®­ç»ƒå¥½çš„å‚æ•°
    torch.save({'W': W, 'b': b}, 'd:/Code/Temp/python/softmax_params.pth')
    print(f"\nå‚æ•°å·²ä¿å­˜åˆ°: d:/Code/Temp/python/softmax_params.pth")

    # æ‰§è¡Œé¢„æµ‹
    predict(net, test_iter)

# åŠ è½½è®­ç»ƒå¥½çš„å‚æ•°
def load_model_params(filepath='d:/Code/Temp/python/softmax_params.pth'):
    """åŠ è½½ä¿å­˜çš„æ¨¡å‹å‚æ•°"""
    try:
        params = torch.load(filepath)
        global W, b
        W = params['W']
        b = params['b']
        print(f"æˆåŠŸåŠ è½½å‚æ•°: {filepath}")
        print(f"æƒé‡ W å½¢çŠ¶: {W.shape}")
        print(f"åç½® b å½¢çŠ¶: {b.shape}")
        return True
    except FileNotFoundError:
        print(f"å‚æ•°æ–‡ä»¶ä¸å­˜åœ¨: {filepath}")
        return False

# ä½¿ç”¨ç¤ºä¾‹ï¼š
# if load_model_params():
#     # å‚æ•°åŠ è½½æˆåŠŸï¼Œå¯ä»¥ç›´æ¥ä½¿ç”¨æ¨¡å‹è¿›è¡Œé¢„æµ‹
#     predict(net, test_iter)
# else:
#     # å‚æ•°æ–‡ä»¶ä¸å­˜åœ¨ï¼Œéœ€è¦é‡æ–°è®­ç»ƒ
#     print("éœ€è¦é‡æ–°è®­ç»ƒæ¨¡å‹...")

# æ·»åŠ ä¸€ä¸ªå‡½æ•°æ¥æŸ¥çœ‹è®­ç»ƒè¿›åº¦
def train_with_progress(net, train_iter, test_iter, loss, num_epochs, updater):
    """å¸¦è¿›åº¦æ˜¾ç¤ºçš„è®­ç»ƒå‡½æ•°"""
    print("ğŸš€ å¼€å§‹è®­ç»ƒ...")
    for epoch in range(num_epochs):
        train_metrics = train_epoch_ch3(net, train_iter, loss, updater)
        test_acc = evaluate_accuracy(net, test_iter)
        
        # æ˜¾ç¤ºè¿›åº¦æ¡
        progress = (epoch + 1) / num_epochs
        bar_length = 30
        filled_length = int(bar_length * progress)
        bar = 'â–ˆ' * filled_length + 'â–‘' * (bar_length - filled_length)
        
        print(f'Epoch {epoch + 1:2d}/{num_epochs} |{bar}| '
              f'Loss: {train_metrics[0]:.3f}, '
              f'Train Acc: {train_metrics[1]:.3f}, '
              f'Test Acc: {test_acc:.3f}')
    
    print("âœ… è®­ç»ƒå®Œæˆï¼")

# ä¿®æ”¹è®­ç»ƒè°ƒç”¨ï¼ˆå¯é€‰ä½¿ç”¨ï¼‰
# å¦‚æœæƒ³è¦æ›´å¥½çš„è¿›åº¦æ˜¾ç¤ºï¼Œå¯ä»¥ç”¨è¿™ä¸ªå‡½æ•°æ›¿æ¢ train_ch3
# train_with_progress(net, train_iter, test_iter, cross_entropy_loss, num_epochs, sgd)
